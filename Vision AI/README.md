One exciting application of multimodal AI is **Vision-Language Models** (VLMs).
These models can process and understand the textual and visual data simultaneously. It can also perform advanced vision-language tasks, such as Visual Question Answering (VQA), image captioning, and Text-to-Image search.

The main take aways are :
- 
- VLM architecture
- VLM evaluation strategies


References: 
1. https://encord.com/blog/vision-language-models-guide/
2. 
 
