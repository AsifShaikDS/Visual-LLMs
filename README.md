Understanding Visual Language models
-
One exciting application of multimodal AI is **Vision-Language Models** (VLMs).
These models can process and understand the textual and visual data simultaneously. It can also perform advanced vision-language tasks, such as Visual Question Answering (VQA), image captioning, and Text-to-Image search.

The main take aways are :
- VLM architecture
- VLM evaluation strategies

A `Vision Language Model` is a fusion of vision and natural language models. It ingests pairs of images and their respective description as inputs and trains on those two modalities. Here Vision + Language has their independent roles. The visio part of the model captures spatial features of the model and the language model encodes the information from text.

The data from both modalities will get mapped to each other. For example, if the image contains a bird, the model will learn to associate it with a similar keyword in the text descriptions.

This way, the model will try to understand images and transforms the knowledge into natural language and vice versa.




References: 
1. https://encord.com/blog/vision-language-models-guide/
2. 
 
